{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:5d6wyhyd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b383d6097d7f4bf5bd1c266e7246ee22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.479443…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dazzling-fog-4</strong> at: <a href='https://wandb.ai/ludeksvoboda/finetune-whisper/runs/5d6wyhyd' target=\"_blank\">https://wandb.ai/ludeksvoboda/finetune-whisper/runs/5d6wyhyd</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230822_084108-5d6wyhyd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:5d6wyhyd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37eb0f703504b36be6cf60f4e449a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666859614999794, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/lu/Projects/FinetuneWhisper/wandb/run-20230822_085705-y8dkh3gs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ludeksvoboda/finetune-whisper/runs/y8dkh3gs' target=\"_blank\">atomic-frost-5</a></strong> to <a href='https://wandb.ai/ludeksvoboda/finetune-whisper' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ludeksvoboda/finetune-whisper' target=\"_blank\">https://wandb.ai/ludeksvoboda/finetune-whisper</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ludeksvoboda/finetune-whisper/runs/y8dkh3gs' target=\"_blank\">https://wandb.ai/ludeksvoboda/finetune-whisper/runs/y8dkh3gs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastprogress import master_bar, progress_bar\n",
    "from fastai.vision.all import SimpleNamespace, set_seed\n",
    "import wandb\n",
    "import numpy as np\n",
    "import whisper\n",
    "import torch\n",
    "from sparseml.pytorch.utils import get_prunable_layers, tensor_sparsity\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    seed = 42,\n",
    "    lr = 0.0005,\n",
    "    batch_size = 1,\n",
    "    epochs = 5,\n",
    "    dropout = 0.2,\n",
    "    weight_decay = 0.01\n",
    ")\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "BATCH_SIZE = 2\n",
    "TRAIN_RATE = 0.8\n",
    "\n",
    "AUDIO_MAX_LENGTH = 480000\n",
    "TEXT_MAX_LENGTH = 120\n",
    "run = wandb.init(project=\"finetune-whisper\",entity=\"ludeksvoboda\", config=config, job_type=\"sparsify_test_run\")\n",
    "\n",
    "set_seed(config.seed)\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lu/pythonEnvs/FinetuneWhisper/lib/python3.10/site-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=True' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4083a94e3a4b63b2caf75a123b80e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0013c9805c44c7bcec0e3a8e0fb400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/8829 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "# common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"cs\", split=\"train+validation\", use_auth_token=True)\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"cs\", split=\"train[:10%]\", use_auth_token=True)\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"cs\", split=\"test\", use_auth_token=True)\n",
    "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
    "\n",
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"cs\", task=\"transcribe\")\n",
    "\n",
    "# input_str = common_voice[\"train\"][0][\"sentence\"]\n",
    "# labels = tokenizer(input_str).input_ids\n",
    "# decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "# decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"cs\", task=\"transcribe\")\n",
    "\n",
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "woptions = whisper.DecodingOptions(language=\"cs\", without_timestamps=True)\n",
    "model = whisper.load_model(\"tiny\")\n",
    "wtokenizer = whisper.tokenizer.get_tokenizer(True, language=\"cs\", task=woptions.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, layer) in get_prunable_layers(model):\n",
    "    print(f\"{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JvsSpeechDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dict) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dict = data_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        data_row = self.data_dict[id]\n",
    "\n",
    "        # audio\n",
    "        mel = torch.tensor(data_row['input_features'])\n",
    "\n",
    "        text = data_row['labels']\n",
    "        labels = text[1:]\n",
    "        dec_in_ids = text[:-1]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": mel,\n",
    "            \"labels\": labels,\n",
    "            \"dec_input_ids\": dec_in_ids\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperDataCollatorWhithPadding:\n",
    "    def __call__(sefl, features):\n",
    "        input_ids, labels, dec_input_ids = [], [], []\n",
    "        for f in features:\n",
    "            input_ids.append(f[\"input_ids\"])\n",
    "            labels.append(f[\"labels\"])\n",
    "            dec_input_ids.append(f[\"dec_input_ids\"])\n",
    "\n",
    "        input_ids = torch.concat([input_id[None, :] for input_id in input_ids])\n",
    "        \n",
    "        label_lengths = [len(lab) for lab in labels]\n",
    "        dec_input_ids_length = [len(e) for e in dec_input_ids]\n",
    "        max_label_len = max(label_lengths+dec_input_ids_length)\n",
    "\n",
    "        labels = [np.pad(lab, (0, max_label_len - lab_len), 'constant', constant_values=-100) for lab, lab_len in zip(labels, label_lengths)]\n",
    "        dec_input_ids = [np.pad(e, (0, max_label_len - e_len), 'constant', constant_values=50257) for e, e_len in zip(dec_input_ids, dec_input_ids_length)] # 50257 is eot token id\n",
    "\n",
    "        batch = {\n",
    "            \"labels\": labels,\n",
    "            \"dec_input_ids\": dec_input_ids\n",
    "        }\n",
    "\n",
    "        batch = {k: torch.tensor(np.array(v), requires_grad=False) for k, v in batch.items()}\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = JvsSpeechDataset(common_voice['train'])\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1, collate_fn=WhisperDataCollatorWhithPadding())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 31])\n",
      "torch.Size([2, 80, 3000])\n",
      "torch.Size([2, 31])\n",
      "<|cs|><|transcribe|><|notimestamps|>Musí v mezinárodní politice zasahovat důsledněji a účinněji.<|endoftext|>\n",
      "<|startoftranscript|><|cs|><|transcribe|><|notimestamps|>Musí v mezinárodní politice zasahovat důsledněji a účinněji.\n",
      "<|cs|><|transcribe|><|notimestamps|>Nizozemské úřady dodnes nevyjádřily svůj postoj.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<|startoftranscript|><|cs|><|transcribe|><|notimestamps|>Nizozemské úřady dodnes nevyjádřily svůj postoj.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for i, b in enumerate(loader):\n",
    "    if i > 0:\n",
    "        break\n",
    "    print(b[\"labels\"].shape)\n",
    "    print(b[\"input_ids\"].shape)\n",
    "    print(b[\"dec_input_ids\"].shape)\n",
    "\n",
    "    for token, dec in zip(b[\"labels\"], b[\"dec_input_ids\"]):\n",
    "        token[token == -100] = wtokenizer.eot\n",
    "        text = tokenizer.decode(token, skip_special_tokens=False)\n",
    "        print(text)\n",
    "\n",
    "        dec[dec == -100] = wtokenizer.eot\n",
    "        text = tokenizer.decode(dec, skip_special_tokens=False)\n",
    "        print(text)\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import evaluate\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = CrossEntropyLoss(ignore_index=-100)\n",
    "metrics_wer = evaluate.load(\"wer\")\n",
    "metrics_cer = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() \n",
    "                            if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": config.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() \n",
    "                            if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 09:05:03 sparseml.pytorch.utils.logger INFO     Logging all SparseML modifier-level logs to sparse_logs/22-08-2023_09.05.03.log\n"
     ]
    }
   ],
   "source": [
    "from sparseml.pytorch.optim import ScheduledModifierManager\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, \n",
    "                          lr=config.lr)\n",
    "\n",
    "manager = ScheduledModifierManager.from_yaml('sparsify_recipe.yaml')\n",
    "optimizer = manager.modify(model, optimizer, steps_per_epoch=len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.00% [1/5 03:29&lt;13:59]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='514' class='' max='1888' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      27.22% [514/1888 00:57&lt;02:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###Cut subset of data before testing\n",
    "mb = master_bar(range(manager.max_epochs))\n",
    "for epoch in mb:\n",
    "    for batch in progress_bar(loader, len(loader), parent=mb):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].cuda()\n",
    "\n",
    "        labels = batch[\"labels\"].long().cuda()\n",
    "        dec_input_ids = batch[\"dec_input_ids\"].long().cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            audio_features = model.encoder(input_ids)\n",
    "\n",
    "        out = model.decoder(dec_input_ids, audio_features)\n",
    "        loss = loss_fn(out.view(-1, out.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ##Make wandb log\n",
    "        wandb.log({\"train_loss\": loss})\n",
    "manager.finalize(model)\n",
    "\n",
    "torch.save({'descripiton': \"\"\"Quick training for testsing sarisfication\n",
    "                        \"\"\",\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, '/home/lu/Models/checkpoints/FinetuneWhisper/tiny/' + 'comvoice_subset_sparse_final(' + str(epoch) + ').tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(self, batch, batch_id):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = batch[\"labels\"].long()\n",
    "        dec_input_ids = batch[\"dec_input_ids\"].long()\n",
    "\n",
    "\n",
    "        audio_features = self.model.encoder(input_ids)\n",
    "        out = self.model.decoder(dec_input_ids, audio_features)\n",
    "\n",
    "        loss = self.loss_fn(out.view(-1, out.size(-1)), labels.view(-1))\n",
    "\n",
    "        out[out == -100] = self.tokenizer.eot\n",
    "        labels[labels == -100] = self.tokenizer.eot\n",
    "\n",
    "        o_list, l_list = [], []\n",
    "        for o, l in zip(out, labels):\n",
    "            o = torch.argmax(o, dim=1)\n",
    "            o_list.append(self.tokenizer.decode(o, skip_special_tokens=True))\n",
    "            l_list.append(self.tokenizer.decode(l, skip_special_tokens=True))\n",
    "        cer = self.metrics_cer.compute(references=l_list, predictions=o_list)\n",
    "        wer = self.metrics_wer.compute(references=l_list, predictions=o_list)\n",
    "\n",
    "        self.log(\"val/loss\", loss, on_step=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val/cer\", cer, on_step=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val/wer\", wer, on_step=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return {\n",
    "            \"cer\": cer,\n",
    "            \"wer\": wer,\n",
    "            \"loss\": loss\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinetuneWhisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
