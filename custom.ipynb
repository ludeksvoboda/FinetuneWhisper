{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mludeksvoboda\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/lu/Projects/FinetuneWhisper/wandb/run-20230822_144853-27t45paw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ludeksvoboda/finetune-whisper/runs/27t45paw' target=\"_blank\">silver-pyramid-9</a></strong> to <a href='https://wandb.ai/ludeksvoboda/finetune-whisper' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ludeksvoboda/finetune-whisper' target=\"_blank\">https://wandb.ai/ludeksvoboda/finetune-whisper</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ludeksvoboda/finetune-whisper/runs/27t45paw' target=\"_blank\">https://wandb.ai/ludeksvoboda/finetune-whisper/runs/27t45paw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastprogress import master_bar, progress_bar\n",
    "from fastai.vision.all import SimpleNamespace, set_seed\n",
    "import wandb\n",
    "import numpy as np\n",
    "import whisper\n",
    "import torch\n",
    "from sparseml.pytorch.utils import get_prunable_layers, tensor_sparsity\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    seed = 42,\n",
    "    lr = 0.0005,\n",
    "    batch_size = 1,\n",
    "    epochs = 5,\n",
    "    dropout = 0.2,\n",
    "    weight_decay = 0.01\n",
    ")\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "BATCH_SIZE = 2\n",
    "TRAIN_RATE = 0.8\n",
    "\n",
    "AUDIO_MAX_LENGTH = 480000\n",
    "TEXT_MAX_LENGTH = 120\n",
    "run = wandb.init(project=\"finetune-whisper\",entity=\"ludeksvoboda\", config=config, job_type=\"sparsify_test_run\")\n",
    "\n",
    "set_seed(config.seed)\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lu/pythonEnvs/FinetuneWhisper/lib/python3.10/site-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=True' instead.\n",
      "  warnings.warn(\n",
      "/home/lu/pythonEnvs/FinetuneWhisper/lib/python3.10/site-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=True' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "# common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"cs\", split=\"train+validation\", use_auth_token=True)\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"cs\", split=\"train[:10%]\", use_auth_token=True)\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"cs\", split=\"test\", use_auth_token=True)\n",
    "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
    "\n",
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"cs\", task=\"transcribe\")\n",
    "\n",
    "# input_str = common_voice[\"train\"][0][\"sentence\"]\n",
    "# labels = tokenizer(input_str).input_ids\n",
    "# decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "# decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"cs\", task=\"transcribe\")\n",
    "\n",
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▌                                 | 56.0M/461M [00:20<02:26, 2.90MiB/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Model has been downloaded but the SHA256 checksum does not not match. Please retry loading the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m woptions \u001b[39m=\u001b[39m whisper\u001b[39m.\u001b[39mDecodingOptions(language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcs\u001b[39m\u001b[39m\"\u001b[39m, without_timestamps\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m whisper\u001b[39m.\u001b[39;49mload_model(\u001b[39m\"\u001b[39;49m\u001b[39msmall\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m wtokenizer \u001b[39m=\u001b[39m whisper\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mget_tokenizer(\u001b[39mTrue\u001b[39;00m, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcs\u001b[39m\u001b[39m\"\u001b[39m, task\u001b[39m=\u001b[39mwoptions\u001b[39m.\u001b[39mtask)\n",
      "File \u001b[0;32m~/pythonEnvs/FinetuneWhisper/lib/python3.10/site-packages/whisper/__init__.py:131\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[1;32m    128\u001b[0m     download_root \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mXDG_CACHE_HOME\u001b[39m\u001b[39m\"\u001b[39m, default), \u001b[39m\"\u001b[39m\u001b[39mwhisper\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m _MODELS:\n\u001b[0;32m--> 131\u001b[0m     checkpoint_file \u001b[39m=\u001b[39m _download(_MODELS[name], download_root, in_memory)\n\u001b[1;32m    132\u001b[0m     alignment_heads \u001b[39m=\u001b[39m _ALIGNMENT_HEADS[name]\n\u001b[1;32m    133\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(name):\n",
      "File \u001b[0;32m~/pythonEnvs/FinetuneWhisper/lib/python3.10/site-packages/whisper/__init__.py:85\u001b[0m, in \u001b[0;36m_download\u001b[0;34m(url, root, in_memory)\u001b[0m\n\u001b[1;32m     83\u001b[0m model_bytes \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(download_target, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mread()\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m hashlib\u001b[39m.\u001b[39msha256(model_bytes)\u001b[39m.\u001b[39mhexdigest() \u001b[39m!=\u001b[39m expected_sha256:\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     86\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel has been downloaded but the SHA256 checksum does not not match. Please retry loading the model.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m     )\n\u001b[1;32m     89\u001b[0m \u001b[39mreturn\u001b[39;00m model_bytes \u001b[39mif\u001b[39;00m in_memory \u001b[39melse\u001b[39;00m download_target\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Model has been downloaded but the SHA256 checksum does not not match. Please retry loading the model."
     ]
    }
   ],
   "source": [
    "woptions = whisper.DecodingOptions(language=\"cs\", without_timestamps=True)\n",
    "model = whisper.load_model(\"small\")\n",
    "wtokenizer = whisper.tokenizer.get_tokenizer(True, language=\"cs\", task=woptions.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, layer) in get_prunable_layers(model):\n",
    "    print(f\"{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JvsSpeechDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dict) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dict = data_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        data_row = self.data_dict[id]\n",
    "\n",
    "        # audio\n",
    "        mel = torch.tensor(data_row['input_features'])\n",
    "\n",
    "        text = data_row['labels']\n",
    "        labels = text[1:]\n",
    "        dec_in_ids = text[:-1]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": mel,\n",
    "            \"labels\": labels,\n",
    "            \"dec_input_ids\": dec_in_ids\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperDataCollatorWhithPadding:\n",
    "    def __call__(sefl, features):\n",
    "        input_ids, labels, dec_input_ids = [], [], []\n",
    "        for f in features:\n",
    "            input_ids.append(f[\"input_ids\"])\n",
    "            labels.append(f[\"labels\"])\n",
    "            dec_input_ids.append(f[\"dec_input_ids\"])\n",
    "\n",
    "        input_ids = torch.concat([input_id[None, :] for input_id in input_ids])\n",
    "        \n",
    "        label_lengths = [len(lab) for lab in labels]\n",
    "        dec_input_ids_length = [len(e) for e in dec_input_ids]\n",
    "        max_label_len = max(label_lengths+dec_input_ids_length)\n",
    "\n",
    "        labels = [np.pad(lab, (0, max_label_len - lab_len), 'constant', constant_values=-100) for lab, lab_len in zip(labels, label_lengths)]\n",
    "        dec_input_ids = [np.pad(e, (0, max_label_len - e_len), 'constant', constant_values=50257) for e, e_len in zip(dec_input_ids, dec_input_ids_length)] # 50257 is eot token id\n",
    "\n",
    "        batch = {\n",
    "            \"labels\": labels,\n",
    "            \"dec_input_ids\": dec_input_ids\n",
    "        }\n",
    "\n",
    "        batch = {k: torch.tensor(np.array(v), requires_grad=False) for k, v in batch.items()}\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = JvsSpeechDataset(common_voice['train'])\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1, collate_fn=WhisperDataCollatorWhithPadding())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 31])\n",
      "torch.Size([2, 80, 3000])\n",
      "torch.Size([2, 31])\n",
      "<|cs|><|transcribe|><|notimestamps|>Musí v mezinárodní politice zasahovat důsledněji a účinněji.<|endoftext|>\n",
      "<|startoftranscript|><|cs|><|transcribe|><|notimestamps|>Musí v mezinárodní politice zasahovat důsledněji a účinněji.\n",
      "<|cs|><|transcribe|><|notimestamps|>Nizozemské úřady dodnes nevyjádřily svůj postoj.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<|startoftranscript|><|cs|><|transcribe|><|notimestamps|>Nizozemské úřady dodnes nevyjádřily svůj postoj.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for i, b in enumerate(loader):\n",
    "    if i > 0:\n",
    "        break\n",
    "    print(b[\"labels\"].shape)\n",
    "    print(b[\"input_ids\"].shape)\n",
    "    print(b[\"dec_input_ids\"].shape)\n",
    "\n",
    "    for token, dec in zip(b[\"labels\"], b[\"dec_input_ids\"]):\n",
    "        token[token == -100] = wtokenizer.eot\n",
    "        text = tokenizer.decode(token, skip_special_tokens=False)\n",
    "        print(text)\n",
    "\n",
    "        dec[dec == -100] = wtokenizer.eot\n",
    "        text = tokenizer.decode(dec, skip_special_tokens=False)\n",
    "        print(text)\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import evaluate\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = CrossEntropyLoss(ignore_index=-100)\n",
    "metrics_wer = evaluate.load(\"wer\")\n",
    "metrics_cer = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() \n",
    "                            if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": config.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() \n",
    "                            if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 09:05:03 sparseml.pytorch.utils.logger INFO     Logging all SparseML modifier-level logs to sparse_logs/22-08-2023_09.05.03.log\n"
     ]
    }
   ],
   "source": [
    "from sparseml.pytorch.optim import ScheduledModifierManager\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, \n",
    "                          lr=config.lr)\n",
    "\n",
    "manager = ScheduledModifierManager.from_yaml('sparsify_recipe.yaml')\n",
    "optimizer = manager.modify(model, optimizer, steps_per_epoch=len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.00% [1/5 03:29&lt;13:59]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='514' class='' max='1888' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      27.22% [514/1888 00:57&lt;02:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###Cut subset of data before testing\n",
    "mb = master_bar(range(manager.max_epochs))\n",
    "for epoch in mb:\n",
    "    for batch in progress_bar(loader, len(loader), parent=mb):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].cuda()\n",
    "\n",
    "        labels = batch[\"labels\"].long().cuda()\n",
    "        dec_input_ids = batch[\"dec_input_ids\"].long().cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            audio_features = model.encoder(input_ids)\n",
    "\n",
    "        out = model.decoder(dec_input_ids, audio_features)\n",
    "        loss = loss_fn(out.view(-1, out.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ##Make wandb log\n",
    "        wandb.log({\"train_loss\": loss})\n",
    "manager.finalize(model)\n",
    "\n",
    "torch.save({'descripiton': \"\"\"Quick training for testsing sarisfication\n",
    "                        \"\"\",\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, '/home/lu/Models/checkpoints/FinetuneWhisper/tiny/' + 'comvoice_subset_sparse_final(' + str(epoch) + ').tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(self, batch, batch_id):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = batch[\"labels\"].long()\n",
    "        dec_input_ids = batch[\"dec_input_ids\"].long()\n",
    "\n",
    "\n",
    "        audio_features = self.model.encoder(input_ids)\n",
    "        out = self.model.decoder(dec_input_ids, audio_features)\n",
    "\n",
    "        loss = self.loss_fn(out.view(-1, out.size(-1)), labels.view(-1))\n",
    "\n",
    "        out[out == -100] = self.tokenizer.eot\n",
    "        labels[labels == -100] = self.tokenizer.eot\n",
    "\n",
    "        o_list, l_list = [], []\n",
    "        for o, l in zip(out, labels):\n",
    "            o = torch.argmax(o, dim=1)\n",
    "            o_list.append(self.tokenizer.decode(o, skip_special_tokens=True))\n",
    "            l_list.append(self.tokenizer.decode(l, skip_special_tokens=True))\n",
    "        cer = self.metrics_cer.compute(references=l_list, predictions=o_list)\n",
    "        wer = self.metrics_wer.compute(references=l_list, predictions=o_list)\n",
    "\n",
    "        self.log(\"val/loss\", loss, on_step=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val/cer\", cer, on_step=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val/wer\", wer, on_step=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return {\n",
    "            \"cer\": cer,\n",
    "            \"wer\": wer,\n",
    "            \"loss\": loss\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinetuneWhisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
