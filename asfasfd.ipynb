{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mludeksvoboda\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/lu/Projects/FinetuneWhisper/wandb/run-20230905_084643-t925de7w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ludeksvoboda/finetune-whisper/runs/t925de7w' target=\"_blank\">small_sparsify_full_data</a></strong> to <a href='https://wandb.ai/ludeksvoboda/finetune-whisper' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ludeksvoboda/finetune-whisper' target=\"_blank\">https://wandb.ai/ludeksvoboda/finetune-whisper</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ludeksvoboda/finetune-whisper/runs/t925de7w' target=\"_blank\">https://wandb.ai/ludeksvoboda/finetune-whisper/runs/t925de7w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sparseml.pytorch.optim import ScheduledModifierManager\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from fastai.vision.all import SimpleNamespace, set_seed\n",
    "import wandb\n",
    "import whisper\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict, Audio\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
    "from dataset import JvsSpeechDataset, WhisperDataCollatorWhithPadding\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from os.path import isfile\n",
    "import evaluate\n",
    "from utils import (\n",
    "    create_dirs_if_not_exist,\n",
    "    set_weight_decay,\n",
    "    define_metrics,\n",
    "    compute_metrics,\n",
    ")\n",
    "import os\n",
    "\n",
    "models_dir = os.getenv(\"MODELS\")\n",
    "model_size = \"small\"\n",
    "run_name = f\"{model_size}_sparsify_full_data\"\n",
    "save_dir = f\"{models_dir}/checkpoints/FinetuneWhisper/{model_size}/\"\n",
    "model_to_load = \"small_10e_full_data_(9).tar\"\n",
    "\n",
    "if not isfile(f\"{save_dir}{model_to_load}\"):\n",
    "    raise ValueError(\"Model to load does not exist\")\n",
    "\n",
    "create_dirs_if_not_exist(save_dir)\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    seed=42,\n",
    "    lr=0.0005,\n",
    "    batch_size=2,\n",
    "    epochs=10,\n",
    "    dropout=0.2,\n",
    "    weight_decay=0.01,\n",
    "    acu_steps=128,\n",
    "    sample_rate=16000,\n",
    ")\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"finetune-whisper\",\n",
    "    entity=\"ludeksvoboda\",\n",
    "    config=config,\n",
    "    job_type=run_name,\n",
    "    name=run_name,\n",
    ")\n",
    "\n",
    "set_seed(config.seed)\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"cs\", split=\"train+validation\", token=True\n",
    ")\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"cs\", split=\"test\", token=True\n",
    ")\n",
    "common_voice = common_voice.remove_columns(\n",
    "    [\n",
    "        \"accent\",\n",
    "        \"age\",\n",
    "        \"client_id\",\n",
    "        \"down_votes\",\n",
    "        \"gender\",\n",
    "        \"locale\",\n",
    "        \"path\",\n",
    "        \"segment\",\n",
    "        \"up_votes\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\n",
    "    f\"openai/whisper-{model_size}\"\n",
    ")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\n",
    "    f\"openai/whisper-{model_size}\", language=\"cs\", task=\"transcribe\"\n",
    ")\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    f\"openai/whisper-{model_size}\", language=\"cs\", task=\"transcribe\"\n",
    ")\n",
    "\n",
    "common_voice = common_voice.cast_column(\n",
    "    \"audio\", Audio(sampling_rate=config.sample_rate)\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]\n",
    "    ).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "\n",
    "common_voice = common_voice.map(\n",
    "    prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=4\n",
    ")\n",
    "\n",
    "woptions = whisper.DecodingOptions(language=\"cs\", without_timestamps=True)\n",
    "model = whisper.load_model(model_size)\n",
    "\n",
    "checkpoint = torch.load(f\"{save_dir}{model_to_load}\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "dataset = JvsSpeechDataset(common_voice[\"train\"])\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=config.batch_size, collate_fn=WhisperDataCollatorWhithPadding()\n",
    ")\n",
    "\n",
    "test_dataset = JvsSpeechDataset(common_voice[\"test\"])\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=WhisperDataCollatorWhithPadding(),\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(model_size).encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 80, 3000])\n"
     ]
    }
   ],
   "source": [
    "print((batch[\"input_ids\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = os.getenv(\"MODELS\")\n",
    "model_size = \"small\"\n",
    "save_dir = f\"{models_dir}/checkpoints/FinetuneWhisper/{model_size}/\"\n",
    "model_to_load = \"small_sparsify_full_data_(9).tar\"\n",
    "\n",
    "if not isfile(f\"{save_dir}{model_to_load}\"):\n",
    "    raise ValueError(\"Model to load does not exist\")\n",
    "\n",
    "model = whisper.load_model(model_size)\n",
    "\n",
    "checkpoint = torch.load(f\"{save_dir}{model_to_load}\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.eval().to(device)\n",
    "\n",
    "model = model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.decoder(dec_input_ids, audio_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AudioEncoder' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencoder\n",
      "File \u001b[0;32m~/pythonEnvs/FinetuneWhisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AudioEncoder' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "encoder = model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6728, -0.6728, -0.6728,  ..., -0.6728, -0.6728, -0.6728],\n",
       "         [-0.6728, -0.6728, -0.6728,  ..., -0.6728, -0.6728, -0.6728],\n",
       "         [-0.6728, -0.6728, -0.6728,  ..., -0.6728, -0.6728, -0.6728],\n",
       "         ...,\n",
       "         [-0.6728, -0.6728, -0.6728,  ..., -0.6728, -0.6728, -0.6728],\n",
       "         [-0.6728, -0.6728, -0.6728,  ..., -0.6728, -0.6728, -0.6728],\n",
       "         [-0.6728, -0.6728, -0.6728,  ..., -0.6728, -0.6728, -0.6728]],\n",
       "\n",
       "        [[-0.7202, -0.7202, -0.7202,  ..., -0.7202, -0.7202, -0.7202],\n",
       "         [-0.7202, -0.7202, -0.7202,  ..., -0.7202, -0.7202, -0.7202],\n",
       "         [-0.7202, -0.7202, -0.7202,  ..., -0.7202, -0.7202, -0.7202],\n",
       "         ...,\n",
       "         [-0.7202, -0.7202, -0.7202,  ..., -0.7202, -0.7202, -0.7202],\n",
       "         [-0.7202, -0.7202, -0.7202,  ..., -0.7202, -0.7202, -0.7202],\n",
       "         [-0.7202, -0.7202, -0.7202,  ..., -0.7202, -0.7202, -0.7202]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = model(batch[\"input_ids\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 31])\n"
     ]
    }
   ],
   "source": [
    "print(batch[\"dec_input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1500, 768])\n"
     ]
    }
   ],
   "source": [
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15, 16, 10,  5, 15,  1, 14, 15,  0, 10,  1,  2, 17,  7,  1,  1, 12, 12,\n",
       "          9, 19,  9,  8,  7, 18,  0,  9,  0, 18,  3,  9, 16]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "torch.randint(20, (1, 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinetuneWhisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
